[MODEL_CONFIG]
reward_gamma = 0.99
gae_tau = 0.95
clip_param = 0.2
k_epochs = 4
entropy_reg = 0.01
actor_lr = 0.0003
critic_lr = 0.0003
max_grad_norm = 5
training_strategy = concurrent
actor_hidden_size = 128
critic_hidden_size = 128
action_masking = False
reward_type = global_R
torch_seed = 0
ROLL_OUT_N_STEPS = 100

[TERL_CONFIG]
k_max = 100
rollout_sensitivity = 2.0
loss_state_th = 1e-2
reward_loss_th = 1.0
use_knowledge = False
knowledge_bias_scale = 0.2

[TRAIN_CONFIG]
MAX_EPISODES = 20000
EPISODES_BEFORE_TRAIN = 1
EVAL_EPISODES = 3
EVAL_INTERVAL = 200
reward_scale = 20.0
log_interval = 1
# 对齐基线：更长的评估种子列表（与基线一致步距）
test_seeds = 0,25,50,75,100,125,150,175,200,225,250,275,300,325,350,375,400,425,450,475,500,525,550,575
save_interval = 500

[ENV_CONFIG]
seed = 0
simulation_frequency = 15
duration = 20
policy_frequency = 5
COLLISION_REWARD = 200
HIGH_SPEED_REWARD = 1
HEADWAY_COST = 4
HEADWAY_TIME = 1.2
MERGING_LANE_COST = 4
traffic_density = 2
